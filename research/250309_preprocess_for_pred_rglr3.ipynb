{"cells":[{"cell_type":"markdown","metadata":{"id":"hMv-mcfb1cxL"},"source":["## 요약\n","- 기존 전처리 코드에서는 정수 인코딩한 token들도 scaling을 진행함\n","- transformer에서 embedding model을 사용하려면 정수 값으로 입력 받아야 함\n","- 또한 중고 도서 데이터가 아닌 도서 정보만 사용\n","- 이에 맞게 코드 수정"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Rd3woP61cxV"},"outputs":[],"source":["import os, natsort, re\n","from tqdm import tqdm\n","import time, random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Jdm14GU9JE1n"},"outputs":[],"source":["from itertools import repeat, chain\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_absolute_percentage_error as mape\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import mean_squared_log_error as msle\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\")\n","# 테마 설정: \"default\", \"classic\", \"dark_background\", \"fivethirtyeight\", \"seaborn\"\n","mpl.style.use(\"fivethirtyeight\")\n","# 이미지가 레이아웃 안으로 들어오도록 함\n","mpl.rcParams.update({\"figure.constrained_layout.use\": True})\n","mpl.rcParams['axes.unicode_minus'] = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":122677,"status":"error","timestamp":1721614229201,"user":{"displayName":"이스트캠퍼스","userId":"11631653647711445405"},"user_tz":-540},"id":"vRSxgFv_1eJE","outputId":"de14af6d-3a18-47a0-8d18-a2e8faa2b8af"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive/\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tOybaY2Uo27a"},"outputs":[],"source":["#cd /content/drive/MyDrive/AI3_prjct2_aladin/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQwX5d0D2H8a","outputId":"975fee49-7127-4362-c443-3165e8b76dc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/transformer_models/research\n"]}],"source":["cd /content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fPd7uoaa1cxU"},"outputs":[],"source":["# 로컬에서\n","\n","plt.rc(\"font\", family = \"D2Coding\")\n","plt.rcParams[\"axes.unicode_minus\"] = False"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZFuEjgvk1cxW"},"outputs":[],"source":["PRJCT_PATH = '/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/aladin_book_price/'\n","#PRJCT_PATH = '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","#PRJCT_PATH = '/content/drive/MyDrive/AI3_prjct2_aladin/aladin_usedbook/'\n","save_dir = 'processed/model_input'\n","dir_path = os.path.join(PRJCT_PATH,save_dir)\n","#dir_path = './'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI20x4AkwggP","outputId":"2e1482eb-d760-46a3-b32c-97cdf2f2b10b"},"outputs":[{"name":"stdout","output_type":"stream","text":["20250211_reg_simulation.ipynb:Zone.Identifier\n","240710_crawling_step0.ipynb\n","240711_crawling_step1.ipynb\n","240711_preprocess_bookinfo.ipynb\n","240715_encoding_usedinfo.ipynb\n","240716_check_bookinfo.ipynb\n","240716_check_bookinfo2.ipynb\n","240716_encoding_bookinfo.ipynb\n","240717_simple_model_for_sample.ipynb\n","240717_split_and_scale.ipynb\n","240718_step0_by_js.ipynb\n","240719_additional_eda.ipynb\n","240719_simple_model_for_cropped.ipynb\n","240721_GridSearch_for_XGB.ipynb\n","240721_experiment_w_XGB.ipynb\n","240721_hyperparameters_XGB.ipynb\n","241023_basic_model.ipynb\n","241023_preprocess_for_pred_rglr.ipynb\n","241024_basic_torch_model.ipynb\n","250112_modulized_test_20480_init_lr_2.12.ipynb\n","250112_modulized_test_20480_init_lr_2.12.ipynb:Zone.Identifier\n","250112_modulized_test_20480_init_lr_analyze2.ipynb\n","250112_modulized_test_20480_init_lr_analyze2.ipynb:Zone.Identifier\n","250112_modulized_test_plot_rslt.ipynb\n","250112_modulized_test_plot_rslt.ipynb:Zone.Identifier\n","250123_modulized_test_20480_init_lr_test_score.ipynb\n","250123_modulized_test_20480_init_lr_test_score.ipynb:Zone.Identifier\n","250211_reg_simulation.ipynb\n","250308_preprocess_for_pred_rglr2.ipynb\n","250309_onlyEncoderModel.ipynb\n","250309_onlyEncoderModel_option_change.ipynb\n","250309_onlyEncoderModel_option_change2.ipynb\n","250309_preprocess_for_pred_rglr3.ipynb\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EKRTD3PdyQsc"},"outputs":[],"source":["import sys\n","sys.path.append(PRJCT_PATH)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-03-11 12:26:07.903875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-11 12:26:08.408857: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-03-11 12:26:08.536394: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2025-03-11 12:26:08.536424: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2025-03-11 12:26:08.617288: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-11 12:26:10.395523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2025-03-11 12:26:10.395842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2025-03-11 12:26:10.395853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["RSLT_DIR = PRJCT_PATH + 'processed/'\n","\n","bookinfo_name = 'bookinfo_ver{}.pkl'.format(0.75)\n","bookinfo_path = os.path.join(RSLT_DIR,bookinfo_name)\n","\n","sys.path.append(PRJCT_PATH)\n","from module_aladin.file_io import load_pkl, save_pkl\n","from module_aladin.data_process import pd_datetime_2_datenum\n","\n","from konlpy.tag import Mecab\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import itertools\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def set_corpus_size(freq,size_feat,mode):\n","    # 입력받은 mode와 size_feat에 따라 size 크기 결정\n","    if mode == 'uniform':\n","        cond = freq['counts']>=freq['counts'].iloc[size_feat]\n","        size = np.sum(cond)\n","    elif mode =='ths':\n","        cond = freq[freq['counts'] > size_feat]\n","        size = np.sum(cond)\n","    else :\n","        if size_feat == None : size = len(freq)\n","#        elif size_feat > len(data) : size = len(freq)\n","        else : size = size_feat\n","    return size\n","\n","def make_encoding_by_freq(freq,null_val='[PAD]',size_feat=None,mode=None):\n","    #빈도수 기반 정수 인코딩 dict 만들기\n","    # freq : token 별 등장 빈도 (value_count), size_feat : size관련 변수(max_size, ths등), mode : size 결정 방법\n","    df_freq = pd.DataFrame(freq).T\n","    df_freq = df_freq.rename(columns={0:'token',1:'counts'})\n","    temp = df_freq.sort_values(by='counts',ascending=False)\n","    size = set_corpus_size(temp,size_feat,mode)\n","    temp = temp.iloc[:size]\n","    temp['val'] = np.arange(size)+1\n","    temp2 = temp.set_index('token').to_dict()\n","    map_token_encode = temp2['val']\n","    map_token_encode[null_val]=0\n","    return map_token_encode\n","\n","def encode_tokens(map_token,x,oov=True):\n","    oov_val = len(map_token)+1 if oov else 0 \n","    return map_token[x] if x in map_token else oov_val\n","\n","def make_author_encode_map(bookinfo,ths_author):\n","    pvtb = pd.pivot_table(data=bookinfo,index='Author',values='SalesPoint',aggfunc=np.sum)\n","    pvtb = pvtb.sort_values(by='SalesPoint',ascending=False)\n","    author_top_k= pvtb[pvtb['SalesPoint']>=ths_author].index\n","    encode_author = pd.DataFrame({'author' : author_top_k.values,'val':np.arange(1,len(author_top_k)+1)})\n","    encode_author = encode_author.set_index('author')\n","    return encode_author.to_dict()['val']\n","\n","def make_publshr_encode_map(publshr_data,ths_publshr):\n","    stats = publshr_data.value_counts().sort_values(ascending=False)\n","    top_k_val = stats.iloc[ths_publshr]\n","    publshr_top_k = list(stats[stats >= top_k_val].index)\n","    return {\n","        publshr : n+1\n","        for n,publshr in enumerate(publshr_top_k)\n","    }\n","    \n","\n","def make_store_encode_map(store_data):\n","    stores= store_data.value_counts().sort_values(ascending=False)\n","    return {\n","        place : n+1\n","        for n,place in enumerate(stores.index)\n","    }"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["file_name = 'bookinfo_ver{}.csv'.format(1.0)\n","file_path = os.path.join(RSLT_DIR,file_name)\n","bookinfo_raw = pd.read_csv(file_path)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["cols = bookinfo_raw.columns.to_list()\n","x_idxs, y_idx = [0,2,3,4,5,6,9,10], 7\n","x_cols = [cols[i] for i in x_idxs]\n","y_col = cols[y_idx]\n","\n","data_X, data_y  = bookinfo_raw[x_cols], bookinfo_raw[y_col]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['BName', 'BName_sub', 'Author', 'Publshr', 'Author_mul', 'Pdate',\n","       'SalesPoint', 'Category'],\n","      dtype='object')"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617,)"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_data, X_tst, y_data, y_tst = train_test_split(data_X,data_y,test_size=0.2,random_state=329)\n","X_trn, X_vld, y_trn, y_vld = train_test_split(X_data,y_data,test_size=0.2,random_state=329)\n","\n","display(X_trn.columns)\n","display(X_trn.shape, y_trn.shape)\n","display(X_vld.shape, y_vld.shape)\n","display(X_tst.shape, y_tst.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["data_dict = {\n","    'trn': {\n","        'X': X_trn,\n","        'y': y_trn\n","        },\n","    'vld':{\n","        'X': X_vld,\n","        'y': y_vld\n","        },\n","    'tst':{\n","        'X': X_tst,\n","        'y': y_tst\n","        \n","    }\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 101173 entries, 96986 to 69788\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   BName       101172 non-null  object\n"," 1   BName_sub   5760 non-null    object\n"," 2   Author      101173 non-null  object\n"," 3   Publshr     101173 non-null  object\n"," 4   Author_mul  101173 non-null  bool  \n"," 5   Pdate       101173 non-null  int64 \n"," 6   SalesPoint  101173 non-null  int64 \n"," 7   Category    101173 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 6.3+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 25294 entries, 132734 to 43660\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       25294 non-null  object\n"," 1   BName_sub   1434 non-null   object\n"," 2   Author      25294 non-null  object\n"," 3   Publshr     25294 non-null  object\n"," 4   Author_mul  25294 non-null  bool  \n"," 5   Pdate       25294 non-null  int64 \n"," 6   SalesPoint  25294 non-null  int64 \n"," 7   Category    25294 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 1.6+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 31617 entries, 146028 to 115151\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       31617 non-null  object\n"," 1   BName_sub   1813 non-null   object\n"," 2   Author      31617 non-null  object\n"," 3   Publshr     31617 non-null  object\n"," 4   Author_mul  31617 non-null  bool  \n"," 5   Pdate       31617 non-null  int64 \n"," 6   SalesPoint  31617 non-null  int64 \n"," 7   Category    31617 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 2.0+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"}],"source":["display(data_dict['trn']['X'].info())\n","display(data_dict['vld']['X'].info())\n","display(data_dict['tst']['X'].info())"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["mecab = Mecab()\n","tokenizer_basic = lambda x : mecab.morphs(x)\n","#cut_date = lambda x : [x[:2],x[2:4],x[4:6],x[6:]]\n","#apply tokenizer\n","cols_vec = ['Category','BName','BName_sub']\n","cols_freq = ['Author','Publshr']"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["for mode, data in data_dict.items():\n","    bookinfo = data['X']\n","    for col in cols_vec:\n","        bookinfo[col] = bookinfo[col].fillna('').apply(tokenizer_basic)\n","    #bookinfo['Pdate'] = bookinfo['Pdate'].astype(str).apply(cut_date)\n","    bookinfo[['Author_mul','Pdate']] = bookinfo[['Author_mul','Pdate']].astype(int).astype(str)\n","    data_dict[mode]['X'] = bookinfo"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#아래 ths 는 EDA 결과 제가 자의적으로 정한 내용\n","bookinfo = data_dict['trn']['X']\n","ths_author = int(np.round(len(bookinfo)/500)*75)\n","ths_publshr = int(np.round(len(bookinfo)/500)*5)\n","\n","map_author_encode = make_author_encode_map(bookinfo[['Author','SalesPoint']],ths_author)\n","map_publshr_encode = make_publshr_encode_map(bookinfo['Publshr'],ths_publshr)\n","\n","encode_maps = {\n","    'Author' : lambda x : encode_tokens(map_author_encode,x,oov=False),\n","    'Publshr' : lambda x : encode_tokens(map_publshr_encode,x,oov=False),\n","}"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:00<00:00, 13.71it/s]\n"]}],"source":["for col in tqdm(cols_freq):\n","    bookinfo[col] = bookinfo[col].map(encode_maps[col]).astype(str)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#make encoding map\n","cols_tknz= ['BName', 'BName_sub', 'Author', 'Publshr', 'Author_mul', 'Pdate','Category']\n","book_tknzed = bookinfo[cols_tknz].to_dict('series')\n","#book_name, book_subname, category = book_tknzed['BName'], book_tknzed['BName_sub'],book_tknzed['Category']"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["array(['19', '92', '37', ..., '392', '0', '0'], dtype=object)"]},"metadata":{},"output_type":"display_data"}],"source":["display('268' in book_tknzed['Publshr'].values)\n","display(book_tknzed['Publshr'].values)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Author\n","Publshr\n","Author_mul\n","Pdate\n"]}],"source":["cols_num = list(filter(lambda x : x not in cols_vec, cols_tknz))\n","number_slicer = lambda x : [x[max(0,i-2):i] for i in range(len(x),0,-2)[::-1]]\n","for col in cols_num:\n","    print(col)\n","    book_tknzed[col] = book_tknzed[col].apply(number_slicer)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["96986        [19]\n","48605        [92]\n","30428        [37]\n","110065       [65]\n","124342    [1, 62]\n","           ...   \n","128584        [1]\n","10037     [1, 68]\n","70829     [3, 92]\n","79290         [0]\n","69788         [0]\n","Name: Publshr, Length: 101173, dtype: object"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["book_tknzed['Publshr']"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["[array(['0', '0', '0', ..., '0', '0', '0'], dtype='<U2'),\n"," array(['19', '92', '37', ..., '92', '0', '0'], dtype='<U2'),\n"," array(['1', '0', '0', ..., '1', '0', '0'], dtype='<U1'),\n"," array(['20', '15', '05', ..., '11', '03', '21'], dtype='<U2'),\n"," array(['소설', '/', '시', ..., '/', '전문', '서적'], dtype='<U3'),\n"," array(['니세코', '이', '비하인드', ..., '원리', '와', '이론'], dtype='<U15'),\n"," array(['(', '전', '작품', ..., '특별', '판', ')'], dtype='<U13')]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["tkn_vals=[]\n","for col in cols_num:\n","    tkn_vals.append(np.array(list(itertools.chain(*book_tknzed[col].values))))\n","for col in cols_vec:\n","    temp = pd.Series(list(itertools.chain(*book_tknzed[col].values)))\n","    cond = ~(pd.to_numeric(temp,errors='coerce')).isna()\n","    temp[cond] = temp[cond].apply(number_slicer)\n","    temp[~cond] = temp[~cond].apply(lambda x :[x])\n","    tkn_vals.append(np.array(list(itertools.chain(*temp.values))))\n","    \n","    \n","#tokens = np.concatenate(tkn_vals,axis=0)\n","#tokens\n","tkn_vals"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["tokens = np.concatenate(tkn_vals,axis=0)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["tokens_ds = pd.Series(tokens)\n","cond_numeric = ~(pd.to_numeric(tokens_ds,errors='coerce')).isna()\n","\n","tokens_num = tokens[cond_numeric]\n","tokens_str = tokens[~cond_numeric]\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["824863 822353\n","110 31441\n"]}],"source":["print(len(tokens_num),len(tokens_str))\n","print(len(np.unique(tokens_num)),len(np.unique(tokens_str)))"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["array([[     1, 157455],\n","       [     2,   6217],\n","       [     2,  22521],\n","       [     2,  13736],\n","       [     2,  14905],\n","       [     2,  13936],\n","       [     2,  17652],\n","       [     2,  14136],\n","       [     2,  15021],\n","       [     2,  13977],\n","       [     2,  14020],\n","       [     1,  68256],\n","       [     2,  25577],\n","       [     2,  17373],\n","       [     2,  16984],\n","       [     2,   8453],\n","       [     2,   8460],\n","       [     2,  14468],\n","       [     2,   8385],\n","       [     2,   8649],\n","       [     2,   8872],\n","       [     2,  16041],\n","       [     1,  19417],\n","       [     2, 116666],\n","       [     2,   8538],\n","       [     2,   8723],\n","       [     2,   8377],\n","       [     2,   6275],\n","       [     2,   8251],\n","       [     2,   3384],\n","       [     2,   3847],\n","       [     2,   4150],\n","       [     2,   3310],\n","       [     1,  13618],\n","       [     2,   8444],\n","       [     2,   3357],\n","       [     2,   1069],\n","       [     2,   1027],\n","       [     2,   1048],\n","       [     2,    970],\n","       [     2,   1028],\n","       [     2,   1029],\n","       [     2,    875],\n","       [     2,   1036],\n","       [     1,   9540],\n","       [     2,   1014],\n","       [     2,    955],\n","       [     2,    936],\n","       [     2,    899],\n","       [     2,    998],\n","       [     2,    888],\n","       [     2,    909],\n","       [     2,    842],\n","       [     2,    888],\n","       [     2,    816],\n","       [     1,   8100],\n","       [     2,   1011],\n","       [     2,    917],\n","       [     2,    917],\n","       [     2,    874],\n","       [     2,    829],\n","       [     2,    875],\n","       [     2,    748],\n","       [     2,    772],\n","       [     2,    774],\n","       [     2,    783],\n","       [     1,   6514],\n","       [     2,    934],\n","       [     2,    793],\n","       [     2,    783],\n","       [     2,    845],\n","       [     2,    838],\n","       [     2,    941],\n","       [     2,    822],\n","       [     2,    834],\n","       [     2,    779],\n","       [     2,    886],\n","       [     1,   5848],\n","       [     2,    731],\n","       [     2,    803],\n","       [     2,    754],\n","       [     2,    760],\n","       [     2,    834],\n","       [     2,    860],\n","       [     2,    785],\n","       [     2,    725],\n","       [     2,    826],\n","       [     2,    654],\n","       [     1,   5250],\n","       [     2,    784],\n","       [     2,    700],\n","       [     2,    742],\n","       [     2,    682],\n","       [     2,    823],\n","       [     2,    815],\n","       [     2,    838],\n","       [     2,    730],\n","       [     2,    781],\n","       [     2,    873],\n","       [     1,   4747],\n","       [     2,    893],\n","       [     2,    921],\n","       [     2,    914],\n","       [     2,   1049],\n","       [     2,   1027],\n","       [     2,   1212],\n","       [     2,   1216],\n","       [     2,   1558],\n","       [     2,   1800],\n","       [     2,   3141]])"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["temp,cnt = np.unique(tokens_num,return_counts=True)\n","temp2 = np.apply_along_axis(lambda x : [(len(n),cnt[i]) for i,n in enumerate(x)],axis=0,arr=temp)\n","temp2"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["np.sum(temp2[:,1][temp2[:,0] < 3])/np.sum(cnt)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["tkns_num = 31000\n","token_freq_str = np.unique(tokens_str,return_counts=True)\n","token_freq_num = np.unique(tokens_num,return_counts=True)\n","map_token_encode_str = make_encoding_by_freq(token_freq_str,size_feat=tkns_num)\n","map_token_encode_num = make_encoding_by_freq(token_freq_num)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["(array(['0', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n","        '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n","        '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n","        '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n","        '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49',\n","        '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59',\n","        '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69',\n","        '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79',\n","        '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n","        '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'],\n","       dtype='<U15'),\n"," array([157455,   6217,  22521,  13736,  14905,  13936,  17652,  14136,\n","         15021,  13977,  14020,  68256,  25577,  17373,  16984,   8453,\n","          8460,  14468,   8385,   8649,   8872,  16041,  19417, 116666,\n","          8538,   8723,   8377,   6275,   8251,   3384,   3847,   4150,\n","          3310,  13618,   8444,   3357,   1069,   1027,   1048,    970,\n","          1028,   1029,    875,   1036,   9540,   1014,    955,    936,\n","           899,    998,    888,    909,    842,    888,    816,   8100,\n","          1011,    917,    917,    874,    829,    875,    748,    772,\n","           774,    783,   6514,    934,    793,    783,    845,    838,\n","           941,    822,    834,    779,    886,   5848,    731,    803,\n","           754,    760,    834,    860,    785,    725,    826,    654,\n","          5250,    784,    700,    742,    682,    823,    815,    838,\n","           730,    781,    873,   4747,    893,    921,    914,   1049,\n","          1027,   1212,   1216,   1558,   1800,   3141]))"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["token_freq_num"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["map_token_encode_num_placed = dict()\n","for key,val in map_token_encode_num.items():\n","    if key == '[PAD]' : continue\n","    map_token_encode_num_placed[key] = val+tkns_num\n","\n","map_token_encode_total = dict(map_token_encode_str,**map_token_encode_num_placed)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["encode_1line =lambda x: list(map(lambda y : encode_tokens(map_token_encode_total,y),x))\n","encode_1line_num =lambda x: list(map(lambda y : encode_tokens(map_token_encode_num,y),x))"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/plain":["[31098, 31112, 31112, 31112]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[98, 112, 112, 112]"]},"metadata":{},"output_type":"display_data"}],"source":["display(encode_1line(['68','268','2341252345','askdlfjwef']))\n","display(encode_1line_num(['68','268','2341252345','askdlfjwef']))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["maxlens={\n","    'Category' : 5,\n","    'BName' : 27,\n","    'BName_sub' : 23,\n","    'Pdate' : 4,\n","    'Publshr' : 2,\n","    'Author' : 2,\n","    'Author_mul' : 1\n","}\n","x_cols = ['BName', 'BName_sub', 'Author',\n","   'Author_mul', 'Publshr', 'Pdate','Category']\n","\n","book_cols = ['BName', 'BName_sub', 'Author', 'Author_mul', 'Publshr', 'Pdate',\n","       'RglPrice', 'SalesPoint', 'Category']\n","xcols_scalar = list(filter(lambda x : x not in maxlens.keys(),x_cols)) "]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["from module_aladin.nlp import erase_space\n","def find_split_num(text):\n","    pat = r'[\\s\\d]+'\n","    mtch_list = list(re.finditer(pat,text))\n","    c,rslt=0,list()\n","    for m in mtch_list:\n","        if c < m.start() : rslt.append((text[c:m.start()],0))\n","        rslt.append((erase_space(text[m.start():m.end()]),1))\n","        c=m.end()\n","    else :\n","        if c < len(text): rslt.append((text[c:len(text)],0))\n","    return rslt\n","\n","\n","def number_splitter(data):\n","    rslt = list()\n","    for txt in data:\n","        temp = find_split_num(txt)\n","        if not temp :rslt.append(temp)\n","        else :\n","            intrmd=list()\n","            for ele in temp:\n","                if ele[1] == 0 : intrmd.append(ele[0])\n","                else : intrmd.extend(number_slicer(ele[0]))\n","            rslt.extend(intrmd)\n","    return rslt        "]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:07<00:00,  1.01s/it]\n","100%|██████████| 7/7 [00:02<00:00,  3.28it/s]\n","100%|██████████| 7/7 [00:02<00:00,  3.40it/s]\n","100%|██████████| 3/3 [00:13<00:00,  4.36s/it]\n"]}],"source":["\n","#encode X\n","X_encoded=dict()\n","for mode,sample in tqdm(data_dict.items()):\n","    X_mode = sample['X'].copy()\n","    #padding and encoding\n","    encoded = pd.DataFrame(index=X_mode.index) \n","    for col in tqdm(x_cols):\n","        if col in cols_num:\n","            X_mode[col] = X_mode[col].apply(number_slicer)\n","        if col in cols_vec:\n","            X_mode[col] = X_mode[col].apply(number_splitter)\n","        \n","        padded = pad_sequences(X_mode[col],padding='post',\n","                                   maxlen=maxlens[col],\n","                                   value='[PAD]',dtype=object)\n","        encoded[col] = list(np.apply_along_axis(encode_1line,0,padded))\n","\n","    concat_tknzed =np.apply_along_axis(np.hstack,1,encoded[list(maxlens.keys())].values)\n","    x_scalar = encoded[xcols_scalar].values\n","    X = np.hstack((concat_tknzed,x_scalar))\n","    X_encoded[mode] = X    "]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"text/plain":["BName         [눈, 이, 아닌, 것, 으로, 도, 읽, 은, 기분]\n","BName_sub                                 []\n","Author                                     0\n","Publshr                                  268\n","Author_mul                                 0\n","Pdate                               20171230\n","SalesPoint                               666\n","Category                               [에세이]\n","Name: 114793, dtype: object"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["data_dict['trn']['X'].iloc[74310]"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"data":{"text/plain":["array([2.2000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       4.3100e+02, 1.5000e+01, 1.3470e+03, 9.7000e+01, 6.9000e+01,\n","       7.5000e+01, 9.1000e+01, 1.1000e+01, 1.9320e+03, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       3.1002e+04, 3.1023e+04, 3.1009e+04, 3.1027e+04, 3.1006e+04,\n","       3.1098e+04, 3.1001e+04, 0.0000e+00, 3.1001e+04])"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded['trn'][74310]"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["('268', True)"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["data_dict['trn']['X'].iloc[74310,3], '268' in data_dict['trn']['X']['Publshr'].values"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'268'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m map_token_encode_num[\u001b[39m'\u001b[39;49m\u001b[39m268\u001b[39;49m\u001b[39m'\u001b[39;49m]\n","\u001b[0;31mKeyError\u001b[0m: '268'"]}],"source":["map_token_encode_num['268']"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"}],"source":["display(np.sum(np.isnan(X_encoded['trn'].astype(np.float64))))\n","display(np.sum(np.isnan(X_encoded['vld'].astype(np.float64))))\n","display(np.sum(np.isnan(X_encoded['tst'].astype(np.float64))))"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["31095\n","31112.0\n","0.0\n"]}],"source":["print(len(np.unique(X_encoded['trn'])))\n","print(np.max(X_encoded['trn']))\n","print(np.min(X_encoded['trn']))"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["X_coded = {\n","    mode : data.astype(np.int32)\n","    for mode, data in X_encoded.items()\n","} "]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","y_coded = defaultdict(dict)\n","for mode,sample in data_dict.items():\n","    y_coded[mode]['value'] = sample['y'].to_numpy()\n","    temp = sample['y'].astype(str).apply(lambda x : number_slicer(x)[::-1])\n","    padded = pad_sequences(temp,padding='post',\n","                                   maxlen=5,\n","                                   value='[PAD]',dtype=object)\n","    intrmd = np.apply_along_axis(encode_1line_num,0,padded)\n","    y_coded[mode]['coded'] = intrmd.astype(np.int32)\n","    y_coded[mode]['decode_map'] = {\n","        v:k\n","        for k,v in map_token_encode_num.items()}\n","    y_coded[mode]['freq'] = np.unique(intrmd.astype(np.int32),return_counts=True)\n","    "]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":["{'value': array([ 6800, 11900, 20000, ..., 14000,  7000, 20000]),\n"," 'coded': array([[ 34,  98,   0,   0,   0],\n","        [ 34,  10,   3,   0,   0],\n","        [ 34,  34,   6,   0,   0],\n","        ...,\n","        [ 34,  56,   3,   0,   0],\n","        [ 34, 105,   0,   0,   0],\n","        [ 34,  34,   6,   0,   0]], dtype=int32),\n"," 'decode_map': {1: '0',\n","  2: '20',\n","  3: '1',\n","  4: '10',\n","  5: '01',\n","  6: '2',\n","  7: '05',\n","  8: '11',\n","  9: '12',\n","  10: '19',\n","  11: '07',\n","  12: '03',\n","  13: '15',\n","  14: '06',\n","  15: '09',\n","  16: '08',\n","  17: '04',\n","  18: '02',\n","  19: '3',\n","  20: '4',\n","  21: '18',\n","  22: '22',\n","  23: '17',\n","  24: '21',\n","  25: '14',\n","  26: '13',\n","  27: '30',\n","  28: '16',\n","  29: '23',\n","  30: '25',\n","  31: '5',\n","  32: '6',\n","  33: '24',\n","  34: '00',\n","  35: '7',\n","  36: '8',\n","  37: '9',\n","  38: '28',\n","  39: '27',\n","  40: '26',\n","  41: '31',\n","  42: '29',\n","  43: '99',\n","  44: '98',\n","  45: '97',\n","  46: '96',\n","  47: '95',\n","  48: '32',\n","  49: '93',\n","  50: '34',\n","  51: '39',\n","  52: '37',\n","  53: '36',\n","  54: '94',\n","  55: '33',\n","  56: '40',\n","  57: '50',\n","  58: '44',\n","  59: '35',\n","  60: '41',\n","  61: '65',\n","  62: '42',\n","  63: '60',\n","  64: '91',\n","  65: '51',\n","  66: '52',\n","  67: '92',\n","  68: '46',\n","  69: '43',\n","  70: '90',\n","  71: '45',\n","  72: '48',\n","  73: '69',\n","  74: '38',\n","  75: '55',\n","  76: '53',\n","  77: '89',\n","  78: '75',\n","  79: '63',\n","  80: '47',\n","  81: '64',\n","  82: '86',\n","  83: '67',\n","  84: '74',\n","  85: '54',\n","  86: '78',\n","  87: '84',\n","  88: '66',\n","  89: '49',\n","  90: '85',\n","  91: '71',\n","  92: '61',\n","  93: '76',\n","  94: '80',\n","  95: '59',\n","  96: '62',\n","  97: '88',\n","  98: '68',\n","  99: '58',\n","  100: '57',\n","  101: '73',\n","  102: '72',\n","  103: '56',\n","  104: '82',\n","  105: '70',\n","  106: '87',\n","  107: '77',\n","  108: '81',\n","  109: '83',\n","  110: '79',\n","  0: '[PAD]'},\n"," 'freq': (array([  0,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n","          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n","          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n","          40,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n","          54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n","          67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n","          80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n","          93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n","         106, 107, 108, 109, 110], dtype=int32),\n","  array([236795,   9688,  49952,   2824,      2,  11002,    104,     27,\n","             90,    124,      1,      1,    331,      2,     57,    200,\n","              4,      1,   3633,   1106,    274,     23,     12,      2,\n","             19,     28,   7363,     14,     10,    645,    439,    232,\n","             16, 109727,    104,     84,     56,    831,     19,     17,\n","            192,    248,   2301,     40,     32,   1702,     35,     30,\n","             10,    160,     15,     80,     13,     39,   4653,  12351,\n","             38,   1806,      2,   1706,    432,   6664,      1,      1,\n","            145,     13,     15,     66,   5107,   2624,   1890,    176,\n","           1754,   2395,     20,    245,   1629,     23,     13,     15,\n","             13,     30,      9,     11,    680,     19,     38,    206,\n","           1861,      2,      2,     34,   8243,    509,     10,    658,\n","           1993,    833,     11,      9,     54,     23,     18,   5789,\n","             40,     40,      2,     17,    171]))}"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["y_coded['trn']"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["data_type = 'encodedXy'\n","strat=0\n","ver=2.0\n","dir_path = os.path.join(RSLT_DIR,'model_input')\n","for mode, x_scaled in X_coded.items():\n","    save_pkl(dir_path,'{}.v{}_st-{}_X_{}.pkl'.format(data_type,ver,strat,mode),x_scaled)\n","for mode,data in y_coded.items(): \n","    save_pkl(dir_path,'{}.v{}_st-{}_y_{}.pkl'.format(data_type,ver,strat,mode),data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
