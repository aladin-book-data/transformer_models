{"cells":[{"cell_type":"markdown","metadata":{"id":"hMv-mcfb1cxL"},"source":["## 요약\n","- 기존 전처리 코드에서는 정수 인코딩한 token들도 scaling을 진행함\n","- transformer에서 embedding model을 사용하려면 정수 값으로 입력 받아야 함\n","- 또한 중고 도서 데이터가 아닌 도서 정보만 사용\n","- 이에 맞게 코드 수정"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Rd3woP61cxV"},"outputs":[],"source":["import os, natsort, re\n","from tqdm import tqdm\n","import time, random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Jdm14GU9JE1n"},"outputs":[],"source":["from itertools import repeat, chain\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_absolute_percentage_error as mape\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import mean_squared_log_error as msle\n","\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats(\"png2x\")\n","# 테마 설정: \"default\", \"classic\", \"dark_background\", \"fivethirtyeight\", \"seaborn\"\n","mpl.style.use(\"fivethirtyeight\")\n","# 이미지가 레이아웃 안으로 들어오도록 함\n","mpl.rcParams.update({\"figure.constrained_layout.use\": True})\n","mpl.rcParams['axes.unicode_minus'] = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":122677,"status":"error","timestamp":1721614229201,"user":{"displayName":"이스트캠퍼스","userId":"11631653647711445405"},"user_tz":-540},"id":"vRSxgFv_1eJE","outputId":"de14af6d-3a18-47a0-8d18-a2e8faa2b8af"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive/\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tOybaY2Uo27a"},"outputs":[],"source":["#cd /content/drive/MyDrive/AI3_prjct2_aladin/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQwX5d0D2H8a","outputId":"975fee49-7127-4362-c443-3165e8b76dc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/transformer_models/research\n"]}],"source":["cd /content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fPd7uoaa1cxU"},"outputs":[],"source":["# 로컬에서\n","\n","plt.rc(\"font\", family = \"D2Coding\")\n","plt.rcParams[\"axes.unicode_minus\"] = False"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZFuEjgvk1cxW"},"outputs":[],"source":["PRJCT_PATH = '/home/doeun/code/AI/ESTSOFT2024/workspace/2.project_text/aladin_book_price/'\n","#PRJCT_PATH = '/content/drive/MyDrive/WASSUP-ESTsoft-AI/project/project2/'\n","#PRJCT_PATH = '/content/drive/MyDrive/AI3_prjct2_aladin/aladin_usedbook/'\n","save_dir = 'processed/model_input'\n","dir_path = os.path.join(PRJCT_PATH,save_dir)\n","#dir_path = './'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI20x4AkwggP","outputId":"2e1482eb-d760-46a3-b32c-97cdf2f2b10b"},"outputs":[{"name":"stdout","output_type":"stream","text":["20250211_reg_simulation.ipynb:Zone.Identifier\n","240710_crawling_step0.ipynb\n","240711_crawling_step1.ipynb\n","240711_preprocess_bookinfo.ipynb\n","240715_encoding_usedinfo.ipynb\n","240716_check_bookinfo.ipynb\n","240716_check_bookinfo2.ipynb\n","240716_encoding_bookinfo.ipynb\n","240717_simple_model_for_sample.ipynb\n","240717_split_and_scale.ipynb\n","240718_step0_by_js.ipynb\n","240719_additional_eda.ipynb\n","240719_simple_model_for_cropped.ipynb\n","240721_GridSearch_for_XGB.ipynb\n","240721_experiment_w_XGB.ipynb\n","240721_hyperparameters_XGB.ipynb\n","241023_basic_model.ipynb\n","241023_preprocess_for_pred_rglr.ipynb\n","241024_basic_torch_model.ipynb\n","250112_modulized_test_20480_init_lr_2.12.ipynb\n","250112_modulized_test_20480_init_lr_2.12.ipynb:Zone.Identifier\n","250112_modulized_test_20480_init_lr_analyze2.ipynb\n","250112_modulized_test_20480_init_lr_analyze2.ipynb:Zone.Identifier\n","250112_modulized_test_plot_rslt.ipynb\n","250112_modulized_test_plot_rslt.ipynb:Zone.Identifier\n","250123_modulized_test_20480_init_lr_test_score.ipynb\n","250123_modulized_test_20480_init_lr_test_score.ipynb:Zone.Identifier\n","250211_reg_simulation.ipynb\n","250308_preprocess_for_pred_rglr2.ipynb\n","250309_onlyEncoderModel.ipynb\n","250309_onlyEncoderModel_option_change.ipynb\n","250309_onlyEncoderModel_option_change2.ipynb\n","250309_preprocess_for_pred_rglr3.ipynb\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"EKRTD3PdyQsc"},"outputs":[],"source":["import sys\n","sys.path.append(PRJCT_PATH)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-03-10 15:50:52.914416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-03-10 15:50:53.329988: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-03-10 15:50:53.430072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2025-03-10 15:50:53.430106: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2025-03-10 15:50:53.511613: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-10 15:50:55.133336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2025-03-10 15:50:55.133705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2025-03-10 15:50:55.133723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"]}],"source":["RSLT_DIR = PRJCT_PATH + 'processed/'\n","\n","bookinfo_name = 'bookinfo_ver{}.pkl'.format(0.75)\n","bookinfo_path = os.path.join(RSLT_DIR,bookinfo_name)\n","\n","sys.path.append(PRJCT_PATH)\n","from module_aladin.file_io import load_pkl, save_pkl\n","from module_aladin.data_process import pd_datetime_2_datenum\n","\n","from konlpy.tag import Mecab\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import itertools\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def set_corpus_size(freq,size_feat,mode):\n","    # 입력받은 mode와 size_feat에 따라 size 크기 결정\n","    if mode == 'uniform':\n","        cond = freq['counts']>=freq['counts'].iloc[size_feat]\n","        size = np.sum(cond)\n","    elif mode =='ths':\n","        cond = freq[freq['counts'] > size_feat]\n","        size = np.sum(cond)\n","    else :\n","        if size_feat == None : size = len(freq)\n","#        elif size_feat > len(data) : size = len(freq)\n","        else : size = size_feat\n","    return size\n","\n","def make_encoding_by_freq(freq,null_val='[PAD]',size_feat=None,mode=None):\n","    #빈도수 기반 정수 인코딩 dict 만들기\n","    # freq : token 별 등장 빈도 (value_count), size_feat : size관련 변수(max_size, ths등), mode : size 결정 방법\n","    df_freq = pd.DataFrame(freq).T\n","    df_freq = df_freq.rename(columns={0:'token',1:'counts'})\n","    temp = df_freq.sort_values(by='counts',ascending=False)\n","    size = set_corpus_size(temp,size_feat,mode)\n","    temp = temp.iloc[:size]\n","    temp['val'] = np.arange(size)+1\n","    temp2 = temp.set_index('token').to_dict()\n","    map_token_encode = temp2['val']\n","    map_token_encode[null_val]=0\n","    return map_token_encode\n","\n","def encode_tokens(map_token,x,oov=True):\n","    oov_val = len(map_token)+1 if oov else 0 \n","    return map_token[x] if x in map_token else oov_val\n","\n","def make_author_encode_map(bookinfo,ths_author):\n","    pvtb = pd.pivot_table(data=bookinfo,index='Author',values='SalesPoint',aggfunc=np.sum)\n","    pvtb = pvtb.sort_values(by='SalesPoint',ascending=False)\n","    author_top_k= pvtb[pvtb['SalesPoint']>=ths_author].index\n","    encode_author = pd.DataFrame({'author' : author_top_k.values,'val':np.arange(1,len(author_top_k)+1)})\n","    encode_author = encode_author.set_index('author')\n","    return encode_author.to_dict()['val']\n","\n","def make_publshr_encode_map(publshr_data,ths_publshr):\n","    stats = publshr_data.value_counts().sort_values(ascending=False)\n","    top_k_val = stats.iloc[ths_publshr]\n","    publshr_top_k = list(stats[stats >= top_k_val].index)\n","    return {\n","        publshr : n+1\n","        for n,publshr in enumerate(publshr_top_k)\n","    }\n","    \n","\n","def make_store_encode_map(store_data):\n","    stores= store_data.value_counts().sort_values(ascending=False)\n","    return {\n","        place : n+1\n","        for n,place in enumerate(stores.index)\n","    }"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["file_name = 'bookinfo_ver{}.csv'.format(1.0)\n","file_path = os.path.join(RSLT_DIR,file_name)\n","bookinfo_raw = pd.read_csv(file_path)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["cols = bookinfo_raw.columns.to_list()\n","x_idxs, y_idx = [0,2,3,4,5,6,9,10], 7\n","x_cols = [cols[i] for i in x_idxs]\n","y_col = cols[y_idx]\n","\n","data_X, data_y  = bookinfo_raw[x_cols], bookinfo_raw[y_col]"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['BName', 'BName_sub', 'Author', 'Publshr', 'Author_mul', 'Pdate',\n","       'SalesPoint', 'Category'],\n","      dtype='object')"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(101173,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(25294,)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617, 8)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(31617,)"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_data, X_tst, y_data, y_tst = train_test_split(data_X,data_y,test_size=0.2,random_state=329)\n","X_trn, X_vld, y_trn, y_vld = train_test_split(X_data,y_data,test_size=0.2,random_state=329)\n","\n","display(X_trn.columns)\n","display(X_trn.shape, y_trn.shape)\n","display(X_vld.shape, y_vld.shape)\n","display(X_tst.shape, y_tst.shape)"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["data_dict = {\n","    'trn': {\n","        'X': X_trn,\n","        'y': y_trn\n","        },\n","    'vld':{\n","        'X': X_vld,\n","        'y': y_vld\n","        },\n","    'tst':{\n","        'X': X_tst,\n","        'y': y_tst\n","        \n","    }\n","}"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 101173 entries, 96986 to 69788\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   BName       101172 non-null  object\n"," 1   BName_sub   5760 non-null    object\n"," 2   Author      101173 non-null  object\n"," 3   Publshr     101173 non-null  object\n"," 4   Author_mul  101173 non-null  bool  \n"," 5   Pdate       101173 non-null  int64 \n"," 6   SalesPoint  101173 non-null  int64 \n"," 7   Category    101173 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 6.3+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 25294 entries, 132734 to 43660\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       25294 non-null  object\n"," 1   BName_sub   1434 non-null   object\n"," 2   Author      25294 non-null  object\n"," 3   Publshr     25294 non-null  object\n"," 4   Author_mul  25294 non-null  bool  \n"," 5   Pdate       25294 non-null  int64 \n"," 6   SalesPoint  25294 non-null  int64 \n"," 7   Category    25294 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 1.6+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 31617 entries, 146028 to 115151\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   BName       31617 non-null  object\n"," 1   BName_sub   1813 non-null   object\n"," 2   Author      31617 non-null  object\n"," 3   Publshr     31617 non-null  object\n"," 4   Author_mul  31617 non-null  bool  \n"," 5   Pdate       31617 non-null  int64 \n"," 6   SalesPoint  31617 non-null  int64 \n"," 7   Category    31617 non-null  object\n","dtypes: bool(1), int64(2), object(5)\n","memory usage: 2.0+ MB\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"}],"source":["display(data_dict['trn']['X'].info())\n","display(data_dict['vld']['X'].info())\n","display(data_dict['tst']['X'].info())"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["mecab = Mecab()\n","tokenizer_basic = lambda x : mecab.morphs(x)\n","#cut_date = lambda x : [x[:2],x[2:4],x[4:6],x[6:]]\n","#apply tokenizer\n","cols_vec = ['Category','BName','BName_sub']\n","cols_freq = ['Author','Publshr']"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["for mode, data in data_dict.items():\n","    bookinfo = data['X']\n","    for col in cols_vec:\n","        bookinfo[col] = bookinfo[col].fillna('').apply(tokenizer_basic)\n","    #bookinfo['Pdate'] = bookinfo['Pdate'].astype(str).apply(cut_date)\n","    bookinfo[['Author_mul','Pdate']] = bookinfo[['Author_mul','Pdate']].astype(int).astype(str)\n","    data_dict[mode]['X'] = bookinfo"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["#아래 ths 는 EDA 결과 제가 자의적으로 정한 내용\n","bookinfo = data_dict['trn']['X']\n","ths_author = int(np.round(len(bookinfo)/500)*75)\n","ths_publshr = int(np.round(len(bookinfo)/500)*5)\n","\n","map_author_encode = make_author_encode_map(bookinfo[['Author','SalesPoint']],ths_author)\n","map_publshr_encode = make_publshr_encode_map(bookinfo['Publshr'],ths_publshr)\n","\n","encode_maps = {\n","    'Author' : lambda x : encode_tokens(map_author_encode,x,oov=False),\n","    'Publshr' : lambda x : encode_tokens(map_publshr_encode,x,oov=False),\n","}"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:00<00:00, 15.08it/s]\n"]}],"source":["for col in tqdm(cols_freq):\n","    bookinfo[col] = bookinfo[col].map(encode_maps[col]).astype(str)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["#make encoding map\n","cols_tknz= ['BName', 'BName_sub', 'Author', 'Publshr', 'Author_mul', 'Pdate','Category']\n","book_tknzed = bookinfo[cols_tknz].to_dict('series')\n","#book_name, book_subname, category = book_tknzed['BName'], book_tknzed['BName_sub'],book_tknzed['Category']"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["array(['19', '92', '37', ..., '392', '0', '0'], dtype=object)"]},"metadata":{},"output_type":"display_data"}],"source":["display('268' in book_tknzed['Publshr'].values)\n","display(book_tknzed['Publshr'].values)"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Author\n","Publshr\n","Author_mul\n","Pdate\n"]}],"source":["cols_num = list(filter(lambda x : x not in cols_vec, cols_tknz))\n","number_slicer = lambda x : [x[max(0,i-2):i] for i in range(len(x),0,-2)[::-1]]\n","for col in cols_num:\n","    print(col)\n","    book_tknzed[col] = book_tknzed[col].apply(number_slicer)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"data":{"text/plain":["96986        [19]\n","48605        [92]\n","30428        [37]\n","110065       [65]\n","124342    [1, 62]\n","           ...   \n","128584        [1]\n","10037     [1, 68]\n","70829     [3, 92]\n","79290         [0]\n","69788         [0]\n","Name: Publshr, Length: 101173, dtype: object"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["book_tknzed['Publshr']"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/plain":["[array(['0', '0', '0', ..., '0', '0', '0'], dtype='<U2'),\n"," array(['19', '92', '37', ..., '92', '0', '0'], dtype='<U2'),\n"," array(['1', '0', '0', ..., '1', '0', '0'], dtype='<U1'),\n"," array(['20', '15', '05', ..., '11', '03', '21'], dtype='<U2'),\n"," array(['소설', '/', '시', ..., '/', '전문', '서적'], dtype='<U3'),\n"," array(['니세코', '이', '비하인드', ..., '원리', '와', '이론'], dtype='<U15'),\n"," array(['(', '전', '작품', ..., '특별', '판', ')'], dtype='<U13')]"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["tkn_vals=[]\n","for col in cols_num:\n","    tkn_vals.append(np.array(list(itertools.chain(*book_tknzed[col].values))))\n","for col in cols_vec:\n","    temp = pd.Series(list(itertools.chain(*book_tknzed[col].values)))\n","    cond = ~(pd.to_numeric(temp,errors='coerce')).isna()\n","    temp[cond] = temp[cond].apply(number_slicer)\n","    temp[~cond] = temp[~cond].apply(lambda x :[x])\n","    tkn_vals.append(np.array(list(itertools.chain(*temp.values))))\n","    \n","    \n","#tokens = np.concatenate(tkn_vals,axis=0)\n","#tokens\n","tkn_vals"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["tokens = np.concatenate(tkn_vals,axis=0)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["tokens_ds = pd.Series(tokens)\n","cond_numeric = ~(pd.to_numeric(tokens_ds,errors='coerce')).isna()\n","\n","tokens_num = tokens[cond_numeric]\n","tokens_str = tokens[~cond_numeric]\n"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["824863 822353\n","110 31441\n"]}],"source":["print(len(tokens_num),len(tokens_str))\n","print(len(np.unique(tokens_num)),len(np.unique(tokens_str)))"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"data":{"text/plain":["array([[     1, 157455],\n","       [     2,   6217],\n","       [     2,  22521],\n","       [     2,  13736],\n","       [     2,  14905],\n","       [     2,  13936],\n","       [     2,  17652],\n","       [     2,  14136],\n","       [     2,  15021],\n","       [     2,  13977],\n","       [     2,  14020],\n","       [     1,  68256],\n","       [     2,  25577],\n","       [     2,  17373],\n","       [     2,  16984],\n","       [     2,   8453],\n","       [     2,   8460],\n","       [     2,  14468],\n","       [     2,   8385],\n","       [     2,   8649],\n","       [     2,   8872],\n","       [     2,  16041],\n","       [     1,  19417],\n","       [     2, 116666],\n","       [     2,   8538],\n","       [     2,   8723],\n","       [     2,   8377],\n","       [     2,   6275],\n","       [     2,   8251],\n","       [     2,   3384],\n","       [     2,   3847],\n","       [     2,   4150],\n","       [     2,   3310],\n","       [     1,  13618],\n","       [     2,   8444],\n","       [     2,   3357],\n","       [     2,   1069],\n","       [     2,   1027],\n","       [     2,   1048],\n","       [     2,    970],\n","       [     2,   1028],\n","       [     2,   1029],\n","       [     2,    875],\n","       [     2,   1036],\n","       [     1,   9540],\n","       [     2,   1014],\n","       [     2,    955],\n","       [     2,    936],\n","       [     2,    899],\n","       [     2,    998],\n","       [     2,    888],\n","       [     2,    909],\n","       [     2,    842],\n","       [     2,    888],\n","       [     2,    816],\n","       [     1,   8100],\n","       [     2,   1011],\n","       [     2,    917],\n","       [     2,    917],\n","       [     2,    874],\n","       [     2,    829],\n","       [     2,    875],\n","       [     2,    748],\n","       [     2,    772],\n","       [     2,    774],\n","       [     2,    783],\n","       [     1,   6514],\n","       [     2,    934],\n","       [     2,    793],\n","       [     2,    783],\n","       [     2,    845],\n","       [     2,    838],\n","       [     2,    941],\n","       [     2,    822],\n","       [     2,    834],\n","       [     2,    779],\n","       [     2,    886],\n","       [     1,   5848],\n","       [     2,    731],\n","       [     2,    803],\n","       [     2,    754],\n","       [     2,    760],\n","       [     2,    834],\n","       [     2,    860],\n","       [     2,    785],\n","       [     2,    725],\n","       [     2,    826],\n","       [     2,    654],\n","       [     1,   5250],\n","       [     2,    784],\n","       [     2,    700],\n","       [     2,    742],\n","       [     2,    682],\n","       [     2,    823],\n","       [     2,    815],\n","       [     2,    838],\n","       [     2,    730],\n","       [     2,    781],\n","       [     2,    873],\n","       [     1,   4747],\n","       [     2,    893],\n","       [     2,    921],\n","       [     2,    914],\n","       [     2,   1049],\n","       [     2,   1027],\n","       [     2,   1212],\n","       [     2,   1216],\n","       [     2,   1558],\n","       [     2,   1800],\n","       [     2,   3141]])"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["temp,cnt = np.unique(tokens_num,return_counts=True)\n","temp2 = np.apply_along_axis(lambda x : [(len(n),cnt[i]) for i,n in enumerate(x)],axis=0,arr=temp)\n","temp2"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["np.sum(temp2[:,1][temp2[:,0] < 3])/np.sum(cnt)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["tkns_num = 31000\n","token_freq_str = np.unique(tokens_str,return_counts=True)\n","token_freq_num = np.unique(tokens_num,return_counts=True)\n","map_token_encode_str = make_encoding_by_freq(token_freq_str,size_feat=tkns_num)\n","map_token_encode_num = make_encoding_by_freq(token_freq_num)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/plain":["(array(['0', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n","        '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n","        '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n","        '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n","        '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49',\n","        '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59',\n","        '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69',\n","        '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79',\n","        '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n","        '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'],\n","       dtype='<U15'),\n"," array([157455,   6217,  22521,  13736,  14905,  13936,  17652,  14136,\n","         15021,  13977,  14020,  68256,  25577,  17373,  16984,   8453,\n","          8460,  14468,   8385,   8649,   8872,  16041,  19417, 116666,\n","          8538,   8723,   8377,   6275,   8251,   3384,   3847,   4150,\n","          3310,  13618,   8444,   3357,   1069,   1027,   1048,    970,\n","          1028,   1029,    875,   1036,   9540,   1014,    955,    936,\n","           899,    998,    888,    909,    842,    888,    816,   8100,\n","          1011,    917,    917,    874,    829,    875,    748,    772,\n","           774,    783,   6514,    934,    793,    783,    845,    838,\n","           941,    822,    834,    779,    886,   5848,    731,    803,\n","           754,    760,    834,    860,    785,    725,    826,    654,\n","          5250,    784,    700,    742,    682,    823,    815,    838,\n","           730,    781,    873,   4747,    893,    921,    914,   1049,\n","          1027,   1212,   1216,   1558,   1800,   3141]))"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["token_freq_num"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["for key,val in map_token_encode_num.items():\n","    if key == '[PAD]' : continue\n","    map_token_encode_num[key] = val+tkns_num\n","\n","map_token_encode_total = dict(map_token_encode_str,**map_token_encode_num)"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["encode_1line =lambda x: list(map(lambda y : encode_tokens(map_token_encode_total,y),x))"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"data":{"text/plain":["[31098, 31112, 31112, 31112]"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["encode_1line(['68','268','2341252345','askdlfjwef'])"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["maxlens={\n","    'Category' : 5,\n","    'BName' : 27,\n","    'BName_sub' : 23,\n","    'Pdate' : 4,\n","    'Publshr' : 2,\n","    'Author' : 2,\n","    'Author_mul' : 1\n","}\n","x_cols = ['BName', 'BName_sub', 'Author',\n","   'Author_mul', 'Publshr', 'Pdate','Category']\n","\n","book_cols = ['BName', 'BName_sub', 'Author', 'Author_mul', 'Publshr', 'Pdate',\n","       'RglPrice', 'SalesPoint', 'Category']\n","xcols_scalar = list(filter(lambda x : x not in maxlens.keys(),x_cols)) "]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["from module_aladin.nlp import erase_space\n","def find_split_num(text):\n","    pat = r'[\\s\\d]+'\n","    mtch_list = list(re.finditer(pat,text))\n","    c,rslt=0,list()\n","    for m in mtch_list:\n","        if c < m.start() : rslt.append((text[c:m.start()],0))\n","        rslt.append((erase_space(text[m.start():m.end()]),1))\n","        c=m.end()\n","    else :\n","        if c < len(text): rslt.append((text[c:len(text)],0))\n","    return rslt\n","\n","\n","def number_splitter(data):\n","    rslt = list()\n","    for txt in data:\n","        temp = find_split_num(txt)\n","        if not temp :rslt.append(temp)\n","        else :\n","            intrmd=list()\n","            for ele in temp:\n","                if ele[1] == 0 : intrmd.append(ele[0])\n","                else : intrmd.extend(number_slicer(ele[0]))\n","            rslt.extend(intrmd)\n","    return rslt        "]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n","100%|██████████| 7/7 [00:01<00:00,  4.77it/s]\n","100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n","100%|██████████| 3/3 [00:10<00:00,  3.57s/it]\n"]}],"source":["\n","#encode X\n","X_encoded=dict()\n","for mode,sample in tqdm(data_dict.items()):\n","    X_mode = sample['X'].copy()\n","    #padding and encoding\n","    encoded = pd.DataFrame(index=X_mode.index) \n","    for col in tqdm(x_cols):\n","        if col in cols_num:\n","            X_mode[col] = X_mode[col].apply(number_slicer)\n","        if col in cols_vec:\n","            X_mode[col] = X_mode[col].apply(number_splitter)\n","        \n","        padded = pad_sequences(X_mode[col],padding='post',\n","                                   maxlen=maxlens[col],\n","                                   value='[PAD]',dtype=object)\n","        encoded[col] = list(np.apply_along_axis(encode_1line,0,padded))\n","\n","    concat_tknzed =np.apply_along_axis(np.hstack,1,encoded[list(maxlens.keys())].values)\n","    x_scalar = encoded[xcols_scalar].values\n","    X = np.hstack((concat_tknzed,x_scalar))\n","    X_encoded[mode] = X    "]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[{"data":{"text/plain":["BName         [눈, 이, 아닌, 것, 으로, 도, 읽, 은, 기분]\n","BName_sub                                 []\n","Author                                     0\n","Publshr                                  268\n","Author_mul                                 0\n","Pdate                               20171230\n","SalesPoint                               666\n","Category                               [에세이]\n","Name: 114793, dtype: object"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["data_dict['trn']['X'].iloc[74310]"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[{"data":{"text/plain":["array([2.2000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       4.3100e+02, 1.5000e+01, 1.3470e+03, 9.7000e+01, 6.9000e+01,\n","       7.5000e+01, 9.1000e+01, 1.1000e+01, 1.9320e+03, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","       3.1002e+04, 3.1023e+04, 3.1009e+04, 3.1027e+04, 3.1006e+04,\n","       3.1098e+04, 3.1001e+04, 0.0000e+00, 3.1001e+04])"]},"execution_count":120,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded['trn'][74310]"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"data":{"text/plain":["('268', True)"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["data_dict['trn']['X'].iloc[74310,3], '268' in data_dict['trn']['X']['Publshr'].values"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'268'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m map_token_encode_num[\u001b[39m'\u001b[39;49m\u001b[39m268\u001b[39;49m\u001b[39m'\u001b[39;49m]\n","\u001b[0;31mKeyError\u001b[0m: '268'"]}],"source":["map_token_encode_num['268']"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0"]},"metadata":{},"output_type":"display_data"}],"source":["display(np.sum(np.isnan(X_encoded['trn'].astype(np.float64))))\n","display(np.sum(np.isnan(X_encoded['vld'].astype(np.float64))))\n","display(np.sum(np.isnan(X_encoded['tst'].astype(np.float64))))"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["31095\n","31112.0\n","0.0\n"]}],"source":["print(len(np.unique(X_encoded['trn'])))\n","print(np.max(X_encoded['trn']))\n","print(np.min(X_encoded['trn']))"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["X_coded = {\n","    mode : data.astype(np.int32)\n","    for mode, data in X_encoded.items()\n","} "]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","y_coded = defaultdict(dict)\n","for mode,sample in data_dict.items():\n","    y_coded[mode]['value'] = sample['y'].to_numpy()\n","    temp = sample['y'].astype(str).apply(lambda x : number_slicer(x)[::-1])\n","    padded = pad_sequences(temp,padding='post',\n","                                   maxlen=5,\n","                                   value='[PAD]',dtype=object)\n","    intrmd = np.apply_along_axis(encode_1line,0,padded)\n","    y_coded[mode]['coded'] = intrmd.astype(np.int32)\n","    y_coded[mode]['decode_map'] = {\n","        v:k\n","        for k,v in map_token_encode_total.items()}\n","    "]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"data":{"text/plain":["{'value': array([ 6800, 11900, 20000, ..., 14000,  7000, 20000]),\n"," 'coded': array([[31034, 31098,     0,     0,     0],\n","        [31034, 31010, 31003,     0,     0],\n","        [31034, 31034, 31006,     0,     0],\n","        ...,\n","        [31034, 31056, 31003,     0,     0],\n","        [31034, 31105,     0,     0,     0],\n","        [31034, 31034, 31006,     0,     0]], dtype=int32),\n"," 'decode_map': {1: '/',\n","  2: '의',\n","  3: '시',\n","  4: '소설',\n","  5: '희곡',\n","  6: '만화',\n","  7: '는',\n","  8: '자격증',\n","  9: '수험서',\n","  10: '과학',\n","  11: '은',\n","  12: '경제',\n","  13: '경영',\n","  14: '인문학',\n","  15: '이',\n","  16: '하',\n","  17: '(',\n","  18: '종교',\n","  19: ')',\n","  20: '역학',\n","  21: '을',\n","  22: '에세이',\n","  23: '한',\n","  24: '사회',\n","  25: ',',\n","  26: '다',\n","  27: '자기',\n","  28: '계발',\n","  29: '-',\n","  30: '교재',\n","  31: '대학',\n","  32: '전문',\n","  33: '가',\n","  34: '서적',\n","  35: '에',\n","  36: '외국어',\n","  37: '역사',\n","  38: '를',\n","  39: '문화',\n","  40: '기',\n","  41: '예술',\n","  42: '컴퓨터',\n","  43: '대중',\n","  44: '나',\n","  45: '.',\n","  46: '+',\n","  47: '권',\n","  48: '들',\n","  49: '모바일',\n","  50: '과',\n","  51: '건강',\n","  52: ':',\n","  53: '좋',\n","  54: '전',\n","  55: '와',\n","  56: '취미',\n","  57: '!',\n","  58: '청소년',\n","  59: '고',\n","  60: '부모',\n","  61: '요리',\n","  62: '지',\n","  63: '살림',\n","  64: '급',\n","  65: '로',\n","  66: '여행',\n","  67: '세트',\n","  68: '내',\n","  69: '으로',\n","  70: '이야기',\n","  71: '있',\n","  72: '사랑',\n","  73: '영어',\n","  74: '게',\n","  75: '도',\n","  76: '에서',\n","  77: '적',\n","  78: '어',\n","  79: '세계',\n","  80: '일',\n","  81: '우리',\n","  82: '모의고사',\n","  83: '기출',\n","  84: '아',\n","  85: '해',\n","  86: '말',\n","  87: '?',\n","  88: '위한',\n","  89: '문제집',\n","  90: '사람',\n","  91: '읽',\n","  92: '제',\n","  93: '판',\n","  94: '아이',\n","  95: '않',\n","  96: '라',\n","  97: '것',\n","  98: '능력',\n","  99: '한국',\n","  100: '만',\n","  101: '필기',\n","  102: '집',\n","  103: '에게',\n","  104: '신',\n","  105: '되',\n","  106: '없',\n","  107: '시험',\n","  108: '년',\n","  109: '인',\n","  110: '책',\n","  111: '법',\n","  112: '사',\n","  113: '실전',\n","  114: '~',\n","  115: '편',\n","  116: '공부',\n","  117: '살',\n","  118: '가지',\n","  119: '상',\n","  120: '서',\n","  121: '보',\n","  122: '주',\n","  123: '님',\n","  124: '특별',\n","  125: '그',\n","  126: '세상',\n","  127: '마음',\n","  128: '할',\n","  129: '&',\n","  130: '투자',\n","  131: '인생',\n","  132: '자',\n","  133: '문제',\n","  134: '교육',\n","  135: '최신',\n","  136: '당신',\n","  137: '면',\n","  138: '기사',\n","  139: '별',\n","  140: '싶',\n","  141: '알',\n","  142: '수',\n","  143: '너',\n","  144: '었',\n","  145: '개',\n","  146: '론',\n","  147: '부',\n","  148: '대',\n","  149: '네',\n","  150: '차',\n","  151: '국어',\n","  152: '여자',\n","  153: '대비',\n","  154: '행복',\n","  155: '엄마',\n","  156: '커스',\n","  157: '더',\n","  158: '윌',\n","  159: '기본',\n","  160: '북',\n","  161: '한다',\n","  162: '는가',\n","  163: '노트',\n","  164: '남자',\n","  165: '시간',\n","  166: '생각',\n","  167: 'NCS',\n","  168: '인간',\n","  169: '실기',\n","  170: '안',\n","  171: '했',\n","  172: '길',\n","  173: '인가',\n","  174: '이론',\n","  175: '습니다',\n","  176: '생활',\n","  177: '형',\n","  178: '공무원',\n","  179: '삶',\n","  180: '여',\n","  181: '시작',\n","  182: '국사',\n","  183: '에듀',\n","  184: '비밀',\n","  185: '속',\n","  186: '시대',\n","  187: '호',\n","  188: '성',\n","  189: '포함',\n","  190: '기초',\n","  191: '배우',\n","  192: '쓰',\n","  193: '왜',\n","  194: '잘',\n","  195: '일본어',\n","  196: '어떻게',\n","  197: '기술',\n","  198: '장',\n","  199: '검사',\n","  200: '철학',\n","  201: '핵심',\n","  202: '토익',\n","  203: '활용',\n","  204: '수업',\n","  205: '번',\n","  206: '세',\n","  207: '오',\n","  208: '부동산',\n","  209: '함께',\n","  210: '성공',\n","  211: '중',\n","  212: '문학',\n","  213: '정리',\n","  214: 'CD',\n","  215: '미래',\n","  216: '꽃',\n","  217: '리',\n","  218: '야',\n","  219: '만들',\n","  220: '끝내',\n","  221: '강의',\n","  222: '수학',\n","  223: '돈',\n","  224: '봉투',\n","  225: '분',\n","  226: '모든',\n","  227: '종',\n","  228: '힘',\n","  229: '성경',\n","  230: '오늘',\n","  231: '스',\n","  232: '는다',\n","  233: '무엇',\n","  234: '화',\n","  235: '상식',\n","  236: '사전',\n","  237: '에디션',\n","  238: '마',\n","  239: '첫',\n","  240: '한국사',\n","  241: '꿈',\n","  242: '검정',\n","  243: '중국',\n","  244: '두',\n","  245: '테이프',\n","  246: '날',\n","  247: '그림',\n","  248: '완성',\n","  249: '처럼',\n","  250: '직무',\n","  251: '부자',\n","  252: '하나님',\n","  253: '합격',\n","  254: '정보',\n","  255: '이해',\n","  256: '기출문제',\n","  257: '죽',\n","  258: '찾',\n","  259: '하루',\n","  260: '학교',\n","  261: '양장',\n","  262: '전쟁',\n","  263: '위',\n","  264: '완전',\n","  265: '가장',\n","  266: '고양이',\n","  267: '나공',\n","  268: '주식',\n","  269: '혁명',\n","  270: '전략',\n","  271: 'TOEIC',\n","  272: '쉽',\n","  273: '원',\n","  274: '장판',\n","  275: '기적',\n","  276: '고전',\n","  277: '회화',\n","  278: '때',\n","  279: '방법',\n","  280: '정판',\n","  281: '적성',\n","  282: '예수',\n","  283: '쉬운',\n","  284: '심리학',\n","  285: '된',\n","  286: '예상',\n","  287: '분석',\n","  288: '한정판',\n","  289: '다시',\n","  290: '산업',\n","  291: '따라',\n","  292: '평가',\n","  293: '한글',\n","  294: '한자',\n","  295: '밤',\n","  296: '현대',\n","  297: '개론',\n","  298: '게임',\n","  299: '선',\n","  300: '면접',\n","  301: '회',\n","  302: '나라',\n","  303: '조선',\n","  304: '왕',\n","  305: '복지',\n","  306: '디자인',\n","  307: '경제학',\n","  308: '달',\n","  309: '초등',\n","  310: '세기',\n","  311: '작',\n","  312: '먹',\n","  313: '보다',\n","  314: '답',\n","  315: '도시',\n","  316: '던',\n","  317: '일본',\n","  318: '몸',\n","  319: '새',\n","  320: '일기',\n","  321: '까지',\n","  322: '행정법',\n","  323: '및',\n","  324: '끝',\n","  325: '대한민국',\n","  326: '커버',\n","  327: '입니다',\n","  328: '법칙',\n","  329: '지도',\n","  330: '아름다운',\n","  331: '합본',\n","  332: '그녀',\n","  333: '직업',\n","  334: '세요',\n","  335: '씨',\n","  336: '소녀',\n","  337: '무료',\n","  338: '생',\n","  339: '비',\n","  340: 'New',\n","  341: '받',\n","  342: '세계사',\n","  343: '박스',\n","  344: '만나',\n","  345: '중국어',\n","  346: '러브',\n","  347: '아빠',\n","  348: '바람',\n","  349: '았',\n","  350: '친구',\n","  351: '습관',\n","  352: '회분',\n","  353: '단기',\n","  354: '기도',\n","  355: '교실',\n","  356: '군',\n","  357: '공인',\n","  358: '애',\n","  359: '총',\n","  360: '연애',\n","  361: '교과서',\n","  362: '겠',\n","  363: '사건',\n","  364: '미국',\n","  365: 'The',\n","  366: '외',\n","  367: '신화',\n","  368: '경찰',\n","  369: ']',\n","  370: '[',\n","  371: '아니',\n","  372: '카드',\n","  373: '지금',\n","  374: '관리',\n","  375: '총론',\n","  376: '선생',\n","  377: '왕자',\n","  378: '교육학',\n","  379: '이기',\n","  380: '워드',\n","  381: '뇌',\n","  382: '일러스트',\n","  383: '미',\n","  384: '입문',\n","  385: 'C',\n","  386: '혼자',\n","  387: '풀',\n","  388: '문법',\n","  389: '아야',\n","  390: '연습',\n","  391: '본',\n","  392: '마케팅',\n","  393: '실무',\n","  394: 'Vol',\n","  395: '후',\n","  396: '공식',\n","  397: '소년',\n","  398: '니',\n","  399: '형법',\n","  400: '하반기',\n","  401: '백',\n","  402: '용',\n","  403: '행정학',\n","  404: 'ACL',\n","  405: '시사',\n","  406: '영',\n","  407: '언어',\n","  408: '직',\n","  409: '이렇게',\n","  410: '매일',\n","  411: '만드',\n","  412: '웹',\n","  413: '최종',\n","  414: '부터',\n","  415: '헌법',\n","  416: '드',\n","  417: '전기',\n","  418: '가족',\n","  419: '최고',\n","  420: '교회',\n","  421: '노',\n","  422: '위대',\n","  423: '처리',\n","  424: '국가',\n","  425: '동영상',\n","  426: '해설',\n","  427: '딸',\n","  428: \"'\",\n","  429: '코',\n","  430: '여름',\n","  431: '눈',\n","  432: '무',\n","  433: '달력',\n","  434: '중개사',\n","  435: '기업',\n","  436: '가이드',\n","  437: '노래',\n","  438: '배',\n","  439: '프로세서',\n","  440: '문',\n","  441: '서울',\n","  442: '검',\n","  443: '미술',\n","  444: '양',\n","  445: '하늘',\n","  446: '죽음',\n","  447: '마왕',\n","  448: '파워',\n","  449: '하나',\n","  450: '회계',\n","  451: '교사',\n","  452: '·',\n","  453: '결혼',\n","  454: '인터넷',\n","  455: '연구',\n","  456: '째',\n","  457: '나무',\n","  458: '합니다',\n","  459: '열',\n","  460: '처음',\n","  461: '관계',\n","  462: '요약',\n","  463: '유아',\n","  464: '영화',\n","  465: '꼭',\n","  466: '공사',\n","  467: '편지',\n","  468: '치',\n","  469: '특강',\n","  470: '일반',\n","  471: '음악',\n","  472: '천',\n","  473: '지식',\n","  474: '아서',\n","  475: '한국어',\n","  476: '숲',\n","  477: 'MP',\n","  478: '건축',\n","  479: '해설집',\n","  480: 'EBS',\n","  481: '무작정',\n","  482: '형사',\n","  483: '학습',\n","  484: '초',\n","  485: '글쓰기',\n","  486: '기능사',\n","  487: '소송법',\n","  488: '밥',\n","  489: '으면',\n","  490: '마법',\n","  491: '그리고',\n","  492: '보이',\n","  493: '며',\n","  494: '관리사',\n","  495: '정치',\n","  496: '기타',\n","  497: '간',\n","  498: '계',\n","  499: '개년',\n","  500: '초보',\n","  501: '고사',\n","  502: '어린이',\n","  503: '유럽',\n","  504: '라이프',\n","  505: '어서',\n","  506: '잡',\n","  507: '불',\n","  508: '교수',\n","  509: '담사',\n","  510: '독서',\n","  511: '도쿄',\n","  512: '을까',\n","  513: '비즈니스',\n","  514: '새로운',\n","  515: '맨',\n","  516: '걸음',\n","  517: '된다',\n","  518: '란',\n","  519: '온라인',\n","  520: '건',\n","  521: '사진',\n","  522: '스페셜',\n","  523: '탄생',\n","  524: '다이어트',\n","  525: '실',\n","  526: '소',\n","  527: '스프링',\n","  528: '걸',\n","  529: '일상',\n","  530: '신부',\n","  531: '손',\n","  532: '누구',\n","  533: '자유',\n","  534: '여성',\n","  535: '난',\n","  536: '미니',\n","  537: '레시피',\n","  538: '거',\n","  539: '맛',\n","  540: '%',\n","  541: '십',\n","  542: '피',\n","  543: '마지막',\n","  544: '거짓말',\n","  545: '월',\n","  546: '독해',\n","  547: '반',\n","  548: '천재',\n","  549: '대하',\n","  550: '앤',\n","  551: '아트',\n","  552: '어라',\n","  553: '저',\n","  554: '유형',\n","  555: '시즌',\n","  556: '우주',\n","  557: 'English',\n","  558: '놀이',\n","  559: '봄',\n","  560: '영웅',\n","  561: '제국',\n","  562: '살인',\n","  563: '묻',\n","  564: '기억',\n","  565: '지혜',\n","  566: '절대',\n","  567: '어느',\n","  568: 'X',\n","  569: '공주',\n","  570: '정신',\n","  571: '그대',\n","  572: '걷',\n","  573: '모르',\n","  574: '과정',\n","  575: '실제',\n","  576: '단어',\n","  577: '종합',\n","  578: '삼성',\n","  579: '바꾸',\n","  580: '스토리',\n","  581: '학원',\n","  582: '이상',\n","  583: 'A',\n","  584: '희망',\n","  585: '선택',\n","  586: '생명',\n","  587: '영혼',\n","  588: '대화',\n","  589: '엑셀',\n","  590: '이름',\n","  591: '마스터',\n","  592: '겨울',\n","  593: '문장',\n","  594: '통합',\n","  595: '괜찮',\n","  596: '지만',\n","  597: '순간',\n","  598: '맛있',\n","  599: 'TOEFL',\n","  600: '트',\n","  601: '사상',\n","  602: '필요',\n","  603: '동형모',\n","  604: '밥상',\n","  605: '원리',\n","  606: '원피스',\n","  607: '경찰학',\n","  608: '빛',\n","  609: '큰',\n","  610: '어떤',\n","  611: '리더십',\n","  612: '포인트',\n","  613: '심리',\n","  614: '악마',\n","  615: '청춘',\n","  616: '피아노',\n","  617: '삼',\n","  618: '어디',\n","  619: '지구',\n","  620: '남',\n","  621: '킹',\n","  622: '터',\n","  623: '천사',\n","  624: '프로그래밍',\n","  625: 'Reading',\n","  626: '기독교',\n","  627: '기념',\n","  628: '너무',\n","  629: '개정판',\n","  630: '그룹',\n","  631: '작가',\n","  632: '결정',\n","  633: 'D',\n","  634: '용사',\n","  635: '리더',\n","  636: '논술',\n","  637: '종말',\n","  638: '입',\n","  639: 's',\n","  640: '명',\n","  641: '디지털',\n","  642: '삼국지',\n","  643: '지역',\n","  644: '모험',\n","  645: '영문법',\n","  646: '서양',\n","  647: '절',\n","  648: '개념',\n","  649: '행동',\n","  650: '지방',\n","  651: '워크',\n","  652: '물',\n","  653: '선물',\n","  654: '회사',\n","  655: '키',\n","  656: '글',\n","  657: '금융',\n","  658: '참',\n","  659: '아라',\n","  660: '산책',\n","  661: '다이어리',\n","  662: '짱',\n","  663: '라는',\n","  664: '머리',\n","  665: '못',\n","  666: '땅',\n","  667: '기행',\n","  668: '티',\n","  669: '근대',\n","  670: '단원',\n","  671: '얼굴',\n","  672: '즈',\n","  673: '강한',\n","  674: '디',\n","  675: '재미있',\n","  676: '온',\n","  677: '천국',\n","  678: '컬러',\n","  679: '진짜',\n","  680: '외전',\n","  681: 'Listening',\n","  682: '카',\n","  683: '시장',\n","  684: '이유',\n","  685: '뉴',\n","  686: '어요',\n","  687: '아침',\n","  688: '교본',\n","  689: '전생',\n","  690: '드래곤',\n","  691: '늑대',\n","  692: '평전',\n","  693: '명탐정',\n","  694: '바로',\n","  695: '동물',\n","  696: '바다',\n","  697: '술',\n","  698: '최',\n","  699: '감정',\n","  700: '어른',\n","  701: '내일',\n","  702: '안녕',\n","  703: '경매',\n","  704: '타',\n","  705: '전공',\n","  706: '바이블',\n","  707: '오브',\n","  708: '아들',\n","  709: '줄',\n","  710: '꿈꾸',\n","  711: '대한',\n","  712: '질문',\n","  713: '사이',\n","  714: '듀',\n","  715: '될',\n","  716: '랑',\n","  717: '루',\n","  718: '사용',\n","  719: '블랙',\n","  720: '중급',\n","  721: '마술',\n","  722: '웃',\n","  723: '단계',\n","  724: '국민',\n","  725: '아버지',\n","  726: '판례',\n","  727: '구조',\n","  728: '발견',\n","  729: '단편',\n","  730: '그리',\n","  731: '같',\n","  732: '색인',\n","  733: '반양장',\n","  734: '트렌드',\n","  735: '위험',\n","  736: '소리',\n","  737: '새벽',\n","  738: '상담',\n","  739: '좀',\n","  740: '믿',\n","  741: '슈퍼',\n","  742: '눈물',\n","  743: '변화',\n","  744: '치료',\n","  745: '육아',\n","  746: 'CEO',\n","  747: '자본주의',\n","  748: '김중근',\n","  749: '고시',\n","  750: '코드',\n","  751: '캐릭터',\n","  752: '설계',\n","  753: '버',\n","  754: '주인',\n","  755: '싫',\n","  756: '전집',\n","  757: '환경',\n","  758: '이것',\n","  759: '월드',\n","  760: '포토샵',\n","  761: '끝장',\n","  762: 'in',\n","  763: '억',\n","  764: '마법사',\n","  765: '선재',\n","  766: '독',\n","  767: '비평',\n","  768: '패턴',\n","  769: '구',\n","  770: '자녀',\n","  771: '세법',\n","  772: '산다',\n","  773: '보건',\n","  774: '많',\n","  775: '마을',\n","  776: '진실',\n","  777: '마녀',\n","  778: '소방',\n","  779: '산',\n","  780: '상처',\n","  781: '어휘',\n","  782: '크',\n","  783: '의사',\n","  784: 'LC',\n","  785: '아기',\n","  786: 'Basic',\n","  787: '숨',\n","  788: '자연',\n","  789: '가이드북',\n","  790: '운동',\n","  791: 'GSAT',\n","  792: '프로',\n","  793: '전동',\n","  794: '위하',\n","  795: '문명',\n","  796: '달인',\n","  797: '식',\n","  798: '듣',\n","  799: '어린',\n","  800: '등',\n","  801: '포트',\n","  802: '한국인',\n","  803: '이란',\n","  804: '완벽',\n","  805: '임용',\n","  806: '재',\n","  807: '쿠',\n","  808: '채용',\n","  809: 'RC',\n","  810: '빵',\n","  811: '바보',\n","  812: '카페',\n","  813: '학',\n","  814: '치유',\n","  815: 'Q',\n","  816: '또',\n","  817: '방',\n","  818: '재테크',\n","  819: '다른',\n","  820: '준',\n","  821: '모두',\n","  822: '전설',\n","  823: '민법',\n","  824: '시크릿',\n","  825: '팔',\n","  826: '프랑스',\n","  827: '유혹',\n","  828: '별매',\n","  829: '엽서',\n","  830: '운명',\n","  831: '프로젝트',\n","  832: '라고',\n","  833: '키우',\n","  834: '엔',\n","  835: '고급',\n","  836: '무선',\n","  837: '어야',\n","  838: '연인',\n","  839: '음식',\n","  840: '여동생',\n","  841: '스타',\n","  842: '즐기',\n","  843: '나이트',\n","  844: '아주',\n","  845: '위기',\n","  846: '고백',\n","  847: '핵',\n","  848: '젊',\n","  849: '클래식',\n","  850: '이런',\n","  851: '파리',\n","  852: '그리스',\n","  853: '쌤',\n","  854: '표현',\n","  855: '필수',\n","  856: '떠나',\n","  857: '사고',\n","  858: '최강',\n","  859: '커피',\n","  860: '동화',\n","  861: '보험',\n","  862: '화학',\n","  863: '간호학',\n","  864: 'Book',\n","  865: '더니',\n","  866: 'All',\n","  867: '볼',\n","  868: '리딩',\n","  869: '키스',\n","  870: '곳',\n","  871: '조직',\n","  872: '단',\n","  873: '멋진',\n","  874: '풍경',\n","  875: '조',\n","  876: '병',\n","  877: '데이터',\n","  878: '붉',\n","  879: '일곱',\n","  880: '펀드',\n","  881: '정석',\n","  882: '그림자',\n","  883: '홈',\n","  884: '훈련',\n","  885: 'E',\n","  886: '공학',\n","  887: '소드',\n","  888: '정복',\n","  889: '사기',\n","  890: 'of',\n","  891: '사례',\n","  892: 'to',\n","  893: '논리',\n","  894: '암기',\n","  895: '슈',\n","  896: '꾼',\n","  897: 'TEPS',\n","  898: '넘',\n","  899: '로드',\n","  900: '라면',\n","  901: '관한',\n","  902: '제공',\n","  903: '인형',\n","  904: '가치',\n","  905: '블루',\n","  906: '바꾼',\n","  907: '나쁜',\n","  908: '실천',\n","  909: '공',\n","  910: '베스트',\n","  911: '그림책',\n","  912: '황제',\n","  913: '이후',\n","  914: '로부터',\n","  915: '센스',\n","  916: '평생',\n","  917: '암',\n","  918: '악',\n","  919: '전효진',\n","  920: '울',\n","  921: '한길',\n","  922: '제자',\n","  923: '이동기',\n","  924: '없이',\n","  925: '보인다',\n","  926: 'B',\n","  927: '파일',\n","  928: '객관식',\n","  929: '교양',\n","  930: '세금',\n","  931: '들려주',\n","  932: '러',\n","  933: '가능',\n","  934: '가을',\n","  935: '독학',\n","  936: '달콤',\n","  937: '박사',\n","  938: '권력',\n","  939: '성장',\n","  940: '잃어버린',\n","  941: '아무',\n","  942: '해도',\n","  943: '길라잡이',\n","  944: '농협',\n","  945: '스피드',\n","  946: '호랑이',\n","  947: '시키',\n","  948: '만든',\n","  949: '빅',\n","  950: '도서관',\n","  951: '하버드',\n","  952: '중문',\n","  953: '환상',\n","  954: '제선',\n","  955: '귀',\n","  956: '취업',\n","  957: '존재',\n","  958: '다면',\n","  959: '신용',\n","  960: '담',\n","  961: '갓',\n","  962: '공간',\n","  963: '아내',\n","  964: '히어로',\n","  965: '이제',\n","  966: '의학',\n","  967: '로운',\n","  968: '움직이',\n","  969: '냐',\n","  970: '하우스',\n","  971: '중학생',\n","  972: '변호사',\n","  973: '도전',\n","  974: '캘린더',\n","  975: '온다',\n","  976: '자수',\n","  977: 'N',\n","  978: '파이널',\n","  979: '초급',\n","  980: '성서',\n","  981: '줘',\n","  982: '어도',\n","  983: '미학',\n","  984: '해석',\n","  985: '블러드',\n","  986: '스타일',\n","  987: '창조',\n","  988: '코믹',\n","  989: '만남',\n","  990: '존',\n","  991: '공기업',\n","  992: '즐거움',\n","  993: '넷',\n","  994: '오래',\n","  995: '기쁨',\n","  996: '열정',\n","  997: '운전',\n","  998: '영원',\n","  999: '기원',\n","  1000: '해야',\n","  ...}}"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["y_coded['trn']"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["data_type = 'encodedXy'\n","strat=0\n","ver=1.5\n","dir_path = os.path.join(RSLT_DIR,'model_input')\n","for mode, x_scaled in X_coded.items():\n","    save_pkl(dir_path,'{}.v{}_st-{}_X_{}.pkl'.format(data_type,ver,strat,mode),x_scaled)\n","for mode,data in y_coded.items(): \n","    save_pkl(dir_path,'{}.v{}_st-{}_y_{}.pkl'.format(data_type,ver,strat,mode),data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
